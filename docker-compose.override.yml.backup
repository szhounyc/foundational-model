services:
  model-trainer:
    networks:
      - contract-review-network
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
    environment:
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

  inference-service:
    networks:
      - contract-review-network
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    environment:
      - PYTHONPATH=/app
      - DEVICE=auto
      - MAX_MEMORY_GB=6
      - USE_FLASH_ATTENTION=false
      - MODELS_DIR=/app/models
      - BASE_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
      - MAX_CONCURRENT_REQUESTS=2
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
      - TOKENIZERS_PARALLELISM=false
      - OMP_NUM_THREADS=4
    command: ["uvicorn", "enhanced_inference:app", "--host", "0.0.0.0", "--port", "9200", "--workers", "1"]

networks:
  contract-review-network:
    external: true
